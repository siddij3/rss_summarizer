t_1 = "From the piece: Earlier this year I decided to take a few weeks to figure out what I think about the existential risk from Artificial Superintelligence (ASI xrisk). It turned out to be much more difficult than I thought. After several months of reading, thinking, and talking with people, what follows is a discussion of a few observations arising during this exploration, including: Here are the passages I thought were interesting enough to tweet about: \"So, what's your probability of doom?\" I think the concept is badly misleading. The outcomes humanity gets depend on choices we can make. We can make choices that make doom almost inevitable, on a timescale of decades – indeed, we don't need ASI for that, we can likely4 arrange it in other ways (nukes, engineered viruses). We can also make choices that make doom extremely unlikely. The trick is to figure out what's likely to lead to flourishing, and to do those things. The term \"probability of doom\" began frustrating me after starting to routinely hear people at AI companies use it fatalistically, ignoring the fact that their choices can change the outcomes. \"Probability of doom\" is an example of a conceptual hazard5 – a case where merely using the concept may lead to mistakes in your thinking. Its main use seems to be as marketing: if widely-respected people say forcefully that they have a high or low probability of doom, that may cause other people to stop and consider why. But I dislike concepts which are good for marketing, but bad for understanding; they foster collective misunderstanding, and are likely to eventually lead to collective errors in action.   With all that said: practical alignment work is extremely accelerationist. If ChatGPT had behaved like Tay, AI would still be getting minor mentions on page 19 of The New York Times. These alignment techniques play a role in AI somewhat like the systems used to control when a nuclear bomb goes off. If such bombs just went off at random, no-one would build nuclear bombs, and there would be no nuclear threat to humanity. Practical alignment work makes today's AI systems far more attractive to customers, far more usable as a platform for building other systems, far more profitable as a target for investors, and far more palatable to governments. The net result is that practical alignment work is accelerationist. There's an extremely thoughtful essay by Paul Christiano, one of the pioneers of both RLHF and AI safety, where he addresses the question of whether he regrets working on RLHF, given the acceleration it has caused. I admire the self-reflection and integrity of the essay, but ultimately I think, like many of the commenters on the essay, that he's only partially facing up to the fact that his work will considerably hasten ASI, including extremely dangerous systems. Over the past decade I've met many AI safety people who speak as though \"AI capabilities\" and \"AI safety/alignment\" work is a dichotomy. They talk in terms of wanting to \"move\" capabilities researchers into alignment. But most concrete alignment work is capabilities work. It's a false dichotomy, and another example of how a conceptual error can lead a field astray. Fortunately, many safety people now understand this, but I still sometimes see the false dichotomy misleading people, sometimes even causing systematic effects through bad funding decisions.   \"Does this mean you oppose such practical work on alignment?\" No! Not exactly. Rather, I'm pointing out an alignment dilemma: do you participate in practical, concrete alignment work, on the grounds that it's only by doing such work that humanity has a chance to build safe systems? Or do you avoid participating in such work, viewing it as accelerating an almost certainly bad outcome, for a very small (or non-existent) improvement in chances the outcome will be good? Note that this dilemma isn't the same as the by-now common assertion that alignment work is intrinsically accelerationist. Rather, it's making a different-albeit-related point, which is that if you take ASI xrisk seriously, then alignment work is a damned-if-you-do-damned-if-you-don't proposition.   \"What are those intrinsic reasons it's hard to make a case either for or against xrisk?\" There are three xrisk persuasion paradoxes that make it difficult. Very briefly, these are:   \"So, when will ASI be able to think its way to new discoveries?\" There's a flipside to the above, which is that ASI can be expected to excel in situations where we already have extremely accurate predictive theories; the contingencies are already known and incorporated into the theory, in detail. Indeed, there are already cases where humanity has used such theories to great advantage to make substantial further \"progress\"32, mostly through thinking (\"theory and/or simulation\") alone, perhaps augmented with a little experiment: There's a flipside to the above, which is that ASI can &gt; be expected to excel in situations where we already &gt; have extremely accurate predictive theories; the &gt;contingencies are already known and incorporated &gt;into the theory\... Even Michael Nielson seems to have a blind spot here, despite all the frankly brilliant and well reasoned arguments prior. Why would \"an ASI\" be limited to only reprocessing existing data? Humans will, once they have ASI grade tools, use some of those tools to do the kinds of tool use tasks that manufactures more robots and chips. This is exponential. With realistically a pool of billions of specialized robots, it is a straightforward task to design a prompt to call an ASI instance to analyze existing experiments and rank possible new experiments by a heuristics of predicted knowledge gain/cost, with respect to some end goal.  (\"Best n experiments for increasing rat longevity\") Then loop it, perform the highest n value experiments across your robotics pool, update your models based on the results, and so on. Hopefully the \"cheap doomsday\" routes Michael is concerned about are too expensive in energy to be practical because if they are not, this kind of experimental loop like above could find it."
